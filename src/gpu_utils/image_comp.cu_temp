

// reduces the values of each thread in a warp down to a single value as described in
// http://devblogs.nvidia.com/parallelforall/faster-parallel-reductions-kepler/
__inline__ __device__ int warpReduceSum(int val) 
{
  for (int offset = warpSize/2; offset > 0; offset /= 2)
    val += __shfl_down(val, offset);
  return val;
}

// If we want to pre-correct the images used for diff2-calculation so that the cuda-kernel does
// not have to store a sigma2-image in shared memory; we can use the below function.
__global__ void cuda_kernel_MinvSigmaCorr(double *img, double *sigma2, double *corr_img)
{
    int n = (blockIdx.x * blockDim.x + threadIdx.x)*2;
    float corr_v = sqrt(*(sigma2 + n)/2);
    *(corr_img + n)     = (*(img + n))      * corr_v;
    *(corr_img + n + 1) = (*(img + n + 1 )) * corr_v;
}


// A function able to use 1D-block geometry AND 1D-thread geometry to copy images into shared memory
__device__  void  cuda_CopyToSharedMem(float* gl_data, int length, float* sh_data)
{
	if(blockIdx.x + threadIdx.x==1)
	{
			int times=length/warpSize;
	}
	for(i=0; i<times; i+=warpSize)
	{
		*(sh_data[blockIdx.x]+i+threadIdx.x) = *(gl_data[blockIdx.x]+i+threadIdx.x)
	}
}

// A function using 1D-block geometry AND 2D-thread geometry to calculate pairwise image diffs
// THIS NOW ASSUMES TO TAKE A DIFF BETWEEN MINVSIGMA2-CORRECTED
__device__ void cuda_SharedMemDiff(float * shA, float * shB, int loadsize, float * shdiff2)
{
	if(blockIdx.x+threadIdx.x==0)
	{
		int times=loadsize/warpSize;
	}
	for(i=0; i<times; i+=warpSize)
	{
		shdiff2[blockIdx.x][threadIdx.y] += warpReduceSum( (*(shA[blockIdx.x]+i+threadIdx.x) - *(shB[threadIdx.y]+i+threadIdx.x))**2 );
	}
}

__global__ void cuda_DiffInSharedMem(float *setA, int NsetA, int Awarps,
                                     float *setB, int NsetB, int Bwarps,
								     int Size_imgs, float *gldiff2)
{
	cudaDeviceProp props;
	cudaGetDeviceProperties(&props, 0);
	#define SharedMemSize = props.sharedMemPerBlock;

	// We want to utilize the maximum 48 warps in an SM, each with 32 threads (=1536 threads)
	// This means factoring 48 as close to integer square root as possible = 6 and 8.
	// If we have 6 vs 8 all-toall comparison we will have 48 independent comparisons.
	// since 6+8=14 we want to fit as much as possible of 14 images into shared memory,
	// so as to have all warps access directly from shared memory as long as possible.
	// With 32 threads in a warp operating on separate pixels, the minimum pixel unit to
	// load is 32. We need to save some space for the 48 resulting floats.

	int shNwarps = Awarps * Bwarps; // the number of comparisons that can be run at once
	int shNimgs =  Awarps + Bwarps; // the number of images to fit into shared memory

	// This is then how many iterations each warp will go thorugh before it's processed as
	// much of the image as we can fit into shared memory
	int loadnum = floor( ((SharedMemSize - sizeof(float)*smNwarps)/smNimgs)/warpSize);

	// And the number of elements to import is precisely $warpSize times this value
	int loadsize = warpSize*loadnum;

	// And the number of partial image loads it will take to complete an entire image
	// Size_img = loadtimes*loadsize + loadrest
	int  loadtimes = ceil(Size_img/(float)loadsize):
	int   loadrest = Size_img - (loadtimes-1)*loadsize;
	int c_loadrest = loadsize - loadrest;
	float zero_pad[c_loadrest];
	for(i=0; i>c_loadrest; i++)
	{
		zero_pad[i]=0;
	}

	//Allocates space for the image parts. We want to make sure that each warp accesses
	//adjacent memory adresses.
	__shared__ float shA[loadsize][Awarps];
	__shared__ float shB[loadsize][Bwarps];

	//Allocate global mem for complete diff2.  //allready allocated and fed into this wrapper-function (?)
	__global__ float[NsetA][NsetB] gldiff2;
	// Allocate shared mem for diff2.
	__shared__ float[Awarps][Bwarps] shdiff2;

	//How many times we will have to load in images from each set
	int Atimes = floor(NsetA/Awarps);
    int Arest = NsetA - Atimes;
	int Btimes = floor(NsetB/Bwarps);
	int Brest = NsetB - Btimes;

	//Temporary vars for the number of images from each subset that are processed in this pass
	int Awarps_t;
	int Bwarps_t;

	// For all times we need to submit $Awarp images from set A
	for(iA=0; iA<=Atimes; iA++)
	{
	    if(iA!=Atimes)
	    	Awarps_t=Awarps;
	    else if(Arest!=0) //The last iteration we reduce the number of images to whatever is left after all the full A subsets of images
	    	Awarps_t=Arest;
		else
			break;

		// For all times we need to compare the current $Awarp images from set A with $Bwarp images from set B
		for(iB=0; iB<=Btimes; iB++)
		{
		    if(iB!=Btimes)
		    	Bwarps_t=Bwarps;
		    else if(Brest!=0) //The last iteration we reduce the number of images to whatever is left after all the full B subsets of images
		    	Bwarps_t=Brest;
			else
				break;

			// For all times we need to load in parts of the current images
			for(j=0; j<loadtimes;j++)
			{
				// TODO FIXME imagepoint objects are not ok. Make sure it is a set of pointers
				//of length Awarps/Bwarps, pointing at the images to be loaded into shared memory
				cuda_CopyToSharedMem<<<Awarps_t,warpSize>>>(setA_imagepoints,  loadsize, *shA[0][:]);
				cuda_CopyToSharedMem<<<Bwarps_t,warpSize>>>(setB_imagepoints,  loadsize, *shB[0][:]);

				if(j==(loadtimes-1) && loadrest!=0) //if this is the last piece of the current images, we will (likely) have loaded in data from other images, which we blank out here
				{
					cuda_CopyToSharedMem<<<Awarps_t,warpSize>>>(zero_pad, c_loadrest, *(shA[0][:])+loadrest);
				}
				// We are now ready to do the comparison of the loaded image pieces of the
				// current $Awarp images from set A, to the current $Bwarp images from set B

				//start $Awarps blocks with $Bwarps warps each, each block handling an image
				//from set A, and each warp in a block handles the comparison to an image in set B

				//example:  8 blocks (8 images from A), each with 6 warps (6 images from B), each with 32 threads = 32*6 = 192 threads per block
				dim3 threadGeomtry(warpSize,Bwarps_t)
				
				// execute difference calculation kernel on the current subset of current image sets
				cuda_SharedMemDiff<<<Awarps_t,threadGeometry>>>(*shA[0][:],*shB[0][:],loadsize,*shdiff2);
			}
			// All image subsets done for the current image sets. Time to copy results back to global.
			// TODO FIXME - THE BELOW IS BASICALLY PSEUDOCODE.
			for(i=0;i<Awarps_t;i++)
			{
				memcpy(shdiff2[i*Bwarps_t],gldiff2[iA*Awarps_t][iB*Bwarps_t],sizeof(float)*Bwarps_t);
			}
		}
	}
}
